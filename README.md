# SiteCrawler

A Python [asyncio](https://docs.python.org/3/library/asyncio.html) website crawler that uses a simple stack:

1. [LMDB](https://lmdb.readthedocs.io/en/release/) for saving downloaded URLs
2. [Celery](https://docs.celeryq.dev/en/stable/index.html) over [Redis](https://redis.io) for work queue management
3. [FastAPI](https://fastapi.tiangolo.com/) provides a thin API wrapper over the crawler 

It was forked from [https://github.com/tapanpandita/aiocrawler](https://github.com/tapanpandita/aiocrawler) (MIT licensed).

It is designed for speed:

- asyncio
- selectolax for fast HTML parsing
- LMDB for high read/write performance
- Unstructured API for file extraction

## Features
- [x] Multiple starting URLs
- [x] Configurable metadata extraction using CSS selectors and regex
- [x] Limit depth of crawl
- [x] Limit number of pages crawled
- [x] Configurable concurrency
- [x] Download sitemaps
- [x] FastAPI API server
- [x] Celery integration
- [x] Dockerized
- [ ] Scheduling crawls
- [x] Return full downloaded content via /browse API
- [ ] Standard webpage metadata extractors
- [x] Integrate with unstructured to extract text from PDF, docx, pptx etc
- [ ] Javascript rendering/parsing via https://splash.readthedocs.io/en/stable/
- [ ] content_css_selector
- [ ] if_modified_since_hours
- [x] Zyte API for automatic data extraction

## Installation

```bash
git clone https://github.com/superkelvint/sitecrawler
cd sitecrawler
docker compose up --build
```

The FastAPI crawler API server is now available at [http://localhost:8000](http://localhost:8000).
The OpenAPI spec is available at [http://localhost:8000/docs](http://localhost:8000/docs).

## Enabling Zyte API for automatic extraction

This step is purely optional. It is only required if you want to utilize Zyte API to automatically extract page Title, Description, Content, and Image. 
```bash
cp config.cfg.sample config.cfg
```
You can then paste your Zyte API key in the config file.


## Starting your first crawl

```bash
curl -X POST http://localhost:8000/crawl \
 -H "Content-Type: application/json" \
 -d '{"name": "supermind", "starting_urls": ["https://www.supermind.org/"], "max_pages": 200, "concurrency": 5, "extraction_rules": {"rules": [{"field_name": "title", "regex": "<title>(.*?)</title>"}]}}'
```

This starts a crawl on supermind.org limited to 200 pages with a concurrency of 5.

The above command should return a UUID response like so:

```bash
{"id":"f339cef3-9839-476d-acdf-b61548515a93"} 
```

This is your job id. You can inspect the result of your crawl like so:

```bash
curl http://localhost:8000/crawl/f339cef3-9839-476d-acdf-b61548515a93
```

After the crawl is completed, you should see a response like:

```json
{
  "id": "f339cef3-9839-476d-acdf-b61548515a93",
  "status": "SUCCESS",
  "info": {
    "name": "supermind",
    "stats": {
      "total": 201,
      "cached": 191,
      "cached_redirects": 2,
      "fetched": 8,
      "new_or_updated": 8
    },
    "start_time": "2024-02-12 05:13:38.358572+0000 (UTC)",
    "end_time": "2024-02-12 05:13:41.063751+0000 (UTC)",
    "duration": "2.705178737640381 seconds"
  }
}  
```


## Design choices
This is not a distributed crawler. The entire crawl runs within a single Celery worker in an async fashion. 
It is very fast, especially if you set a high concurrency. 

Hitting a single host from a single crawler node, we can easily hit 50 URLs/s with concurrency of 10.  

The crawl job runs in 2 phases:
1. download all URLs
2. parse and extract

Each phase runs as a separate Celery task. This means it should be possible to have different workers doing different tasks. 

## APIs

<details>
  <summary><code>GET</code> <code><b>/health</b></code> <code>Health Check</code></summary>

### Response
 ```json
{
  "health": "GREEN"
}
```
</details>

<details>
  <summary><code>POST</code> <code><b>/crawl</b></code> <code>Submit a new crawl</code></summary>

### Request Object
| **Name**                        | **Type**        | **Default**     | **Description**                                                                                                                                                                                                        |
| ------------------------------- | --------------- | --------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **name**                        | string          |                 | Name of the crawl (required)                                                                                                                                                                                           |
| **starting_urls**               | list of strings |                 | Starting URLs (required)                                                                                                                                                                                               |
| **allowed_domains**             | list of strings |                 | Allowed domains. Additive with allow_starting_url_hostname and allow_starting_url_tld.                                                                                                                                 |
| **allowed_regex**               | list of strings |                 | URLs matching with these regexes will be allowed.                                                                                                                                                                      |
| **denied_regex**                | list of strings |                 | URLs matching with these regexes will not be crawled.                                                                                                                                                                  |
| **denied_extensions**           | list of strings |                 | URLs ending with these extensions will not be crawled.                                                                                                                                                                 |
| **is_sitemap**                  | boolean         | false           | If true, all the starting URLs will be treated as sitemaps. The entire sitemaps will be downloaded, all the URLs collected and crawled to a depth of 1. No URLs other than what is in the sitemaps will be downloaded. |
| **max_depth**                   | number          | 300             | Maximum crawler depth. The starting URL is depth of 1.                                                                                                                                                                 |
| **max_pages**                   | number          | -1              | Max number of pages to crawl. -1 means no limit (default)                                                                                                                                                              |
| **concurrency**                 | number          | 10              | Simultaneous crawler connections.                                                                                                                                                                                      |
| **allow_starting_url_hostname** | boolean         | true            | Allow all links with the same hostname as starting URLs.                                                                                                                                                               |
| **allow_starting_url_tld**      | boolean         | false           | Allow all links with the same TLD as starting URLs.                                                                                                                                                                    |
| **user_agent**                  | string          | SiteCrawler/1.0 | Crawler user-agent.                                                                                                                                                                                                    |
| **extraction_rules**            | dictionary      |                 | See ExtractionRules section.                                                                                                                                                                                           |
| **ai_parsing**                  | boolean         | false           | Automatically extract Title, Description, Content, and Image using AI parsing |

#### Example
```json
{
  "name": "supermind",
  "starting_urls": [
    "https://www.supermind.org/"
  ],
  "max_pages": 200,
  "concurrency": 5,
  "extraction_rules": {
    "rules": [
      {
        "field_name": "title",
        "regex": "<title>(.*?)</title>"
      },
      {
        "field_name": "description",
        "css": "meta[name=description]",
        "attribute": "content"
      }
    ]
  }
}
```

### Extraction Rules
| **Name**          | **Type** | **Description**                                                                                                    |
| ----------------- | -------- | ------------------------------------------------------------------------------------------------------------------ |
| **field_name**    | string   | Name of the field                                                                                                  |
| **css**           | string   | CSS selector.                                                                                                      |
| **regex**         | string   | Regex. There must be 1 matching group.                                                                             |
| **delimiter**     | string   | Not currently used.                                                                                                |
| **attribute**     | string   | **CSS only**. If specifed, the HTML element attribute it extracted. Otherwise, the element text is used (default). |
| **fixed_value**   | string   | Populate the field with a fixed value. It can be useful if merging data from different sites                       |
| **default_value** | string   | Default value to use if there is no match                                                                          |

There should only be either `css` or `regex` declared. If both are declared, `css` is used. 
### Response
 ```json
{
  "id": "b67dbbbb-84c0-45c5-b467-85eb846c8988"
}
```
</details>

<details>
  <summary><code>GET</code> <code><b>/crawl</b></code> <code>List all active crawl</code></summary>

### Response
 ```json
{
  "jobs": [
    {
      "id": "1fcfba77-03d4-47d1-9b3d-41ca5336c27d",
      "status": "PENDING",
      "info": null
    }
  ]
}
```
</details>

<details>
  <summary><code>GET</code> <code><b>/crawl/{job_id}</b></code> <code>Get details of crawl by its job ID</code></summary>

### Response
 ```json
{
  "id": "b67dbbbb-84c0-45c5-b467-85eb846c8988",
  "status": "SUCCESS",
  "info": {
    "name": "test",
    "stats": {
      "total": 1,
      "cached": 1
    },
    "start_time": "2024-03-18 17:16:52.552218+0000 (UTC)",
    "end_time": "2024-03-18 17:16:52.583229+0000 (UTC)",
    "duration": "less than a second"
  }
}
```
</details>

<details>
  <summary><code>DELETE</code> <code><b>/crawl/{job_id}</b></code> <code>Stop the crawl</code></summary>

### Response
 ```json
{
  "jobs": [
    {
      "id": "1fcfba77-03d4-47d1-9b3d-41ca5336c27d",
      "status": "PENDING",
      "info": null
    }
  ]
}
```
</details>

<details>
  <summary><code>GET</code> <code><b>/browse/{name}</b></code> <code>Get contents of the crawl</code></summary>

### Parameters
| name        | data type | description              |
| ----------- | --------- | ------------------------ |
| page        | int       | Page number              |
| rows        | int       | Number of items per page |
| fullcontent | bool      | Return unparsed content  |

### Response
 ```json
{
  "name": "test_crawl",
  "page": 0,
  "total_pages": 1,
  "num_records": 1,
  "items": [
    {
      "server_last_modified": "Fri, 16 Feb 2024 15:43:40 GMT",
      "_content": "xyz file content",
      "content_type": "application/vnd.openxmlformats-officedocument.presentationml.presentation",
      "title": "filename.pptx",
      "description": "",
      "metacategory_ss": "",
      "metaproducts_ss": "",
      "metaindustries_ss": "",
      "ogimage_s": "",
      "h1list": "",
      "h2list": "",
      "h3list": "",
      "h4list": "",
      "uri": "https://...REDACTED.../filename.pptx",
      "path_s": "sites / default / files / 2024-02 / filename.pptx",
      "typeUrl_s": "Sites",
      "id": "07794058-2453-3a03-b292-1278da923e3d"
    }
  ]
}
```
</details>






## Using sitecrawler as a library

```python
import asyncio

from sitecrawler import SiteCrawler, do_extraction

if __name__ == "__main__":
    crawler = SiteCrawler("test_crawl", ["https://www.supermind.org"], max_pages=10)
    asyncio.run(crawler.get_results())
    print(crawler.stats)
    do_extraction(crawler)
```

## Using sitecrawler from the command-line
```bash
source ./venv/bin/activate
python3 sitecrawler.py --name=supermind --starting_urls="https://www.supermind.org"
```

## Running celery locally

Celery is not needed to run the sitecrawler. You can run it as-is. If you do wish to run Celery locally, 
in a terminal, this is how to start the celery worker:
```bash
source ./venv/bin/activate
celery -A tasks worker --loglevel=INFO
```

You will also need Redis running locally. 

## Contributing

The 4 files of interest are:

- sitecrawler.py: the main crawler logic
- aiocrawler.py: the crawler base class that handles most of the async fetching operations
- main.py: the FastAPI wrapper around the crawler
- tasks.py: submits jobs to celery
